<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sarthak Yadav</title>
  
  <meta name="author" content="Sarthak Yadav">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/icon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sarthak Yadav</name>
                <br>
                <!-- <font size="3"><br/>Learning to teach machines how to learn.</font> -->
              </p>
              <br/>
              <p align="justify"><font size="3">
              I am an MSc (Research) student at the <a href="https://www.gla.ac.uk/schools/computing/" target="_blank"><font size="3">School of Computing Science</font></a>, University of Glasgow.
              </font></p>
              <p align="justify"><font size="3">
              Previously, I worked as the Lead Research Engineer at <a href="https://www.staqu.com/" target="_blank"><font size="3">Staqu Technologies</font></a>, a premier Indian AI startup, where I led the design and development of several large scale mission-critical intelligent systems, spanning computer vision (such as violence recognition and scalable object detection), spoken language understanding (most prominently speaker biometrics and ASR) and neural machine translation for low-resource languages.
              I also architected and co-developed Staqu's scalable inference system (Shoutout to the Backend Team @ Staqu!), handling several projects with over 10M inferences per day. 
              Before that I worked on several problems on structured data during my bachelors, most prominently Computer Virology, with <a href="https://www.linkedin.com/in/ankur-singh-bist-5256a085" target="_blank"><font size="3">Dr. Ankur Singh Bist</font></a> and <a href="https://www.smvdu.ac.in/index.php/bachelor-of-technology/35-faculty-profile/fp-comp-sc-engg/1223-dr-manoj-kumar-gupta" target="_blank"><font size="3">Dr. Manoj Kumar Gupta</font></a>.
              </font></p>
              <br>
              <p style="text-align:center">
                <a href="mailto:sarthak.yadav3@gmail.com" target="_blank"><font size="3">Email</font></a> &nbsp|&nbsp
                <a href="https://drive.google.com/file/d/12eQcoi2FVLpd0GJmrKSHiuJhC5MSX5S5/view?usp=sharing" target="_blank">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=cVsg9VsAAAAJ&hl=en" target="_blank"><font size="3">Google Scholar</font></a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/sarthakyadav/" target="_blank"><font size="3"> LinkedIn </font></a>  &nbsp|&nbsp
                <a href="https://github.com/SarthakYadav" target="_blank"><font size="3"> GitHub </font></a>  &nbsp|&nbsp
                <a href="https://www.kaggle.com/yadavsarthak" target="_blank"><font size="3"> Kaggle </font></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar_2.jpg" target="_blank"><img style="width:100%;max-width:100%" alt="Profile Photo" src="images/avatar_2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- RESEARCH -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading><font size="5">Research Interests</font></heading>
              <p><font size="3">
                I am interested in the interpretability of raw waveform based audio processing deep neural networks (DNNs) in the discrete-time audio input space, specifically the dynamics of how self-supervised representations differ from supervised ones. I believe that an amalgamation of visual and aural means of interpretation would work best for interpreting such models.
              </font></p>
            </td>
          </tr>
        </tbody></table>

        <!-- PUBLICATIONS -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><font size="5">Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- ICASSP 2020 - Frequency and Temporal Convolutional Attention -->
          <tr onmouseout="icassp20_0_stop()" onmouseover="icassp20_0_start()">
            <td style="padding:25px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icassp20_0_image'><img src='images/pub_icassp20_0_re.jpg'></div>
                <img src='images/pub_icassp20_0_re.jpg'>
              </div>
              <script type="text/javascript">
                function icassp20_0_start() {
                  document.getElementById('icassp20_0_image').style.opacity = "1";
                }

                function icassp20_0_stop() {
                  document.getElementById('icassp20_0_image').style.opacity = "0";
                }
                icassp20_0_stop()
              </script>
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9054440" target="_blank">
                <papertitle><font size="3">Frequency and Temporal Convolutional Attention for Text-Independent Speaker Recognition</font></papertitle>
              </a>
              <br>
              <font size="3">
              <strong><font size="3">Sarthak Yadav</font></strong>,
              <font size="3">Atul Rai</font>
              <br>
              <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('icassp20_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('icassp20_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="icassp20_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Majority of the recent approaches for text-independent speaker recognition apply attention or similar techniques for aggregation of frame-level feature descriptors generated by a deep neural network (DNN) front-end. In this paper, we propose methods of convolutional attention for independently modelling temporal and frequency information in a convolutional neural network (CNN) based front-end. Our system utilizes convolutional block attention modules (CBAMs) [1] appropriately modified to accommodate spectrogram inputs. The proposed CNN front-end fitted with the proposed convolutional attention modules outperform the no-attention and spatial-CBAM baselines by a significant margin on the VoxCeleb [2], [3] speaker verification benchmark. Our best model achieves an equal error rate of 2.031% on the VoxCeleb1 test set, which is a considerable improvement over comparable state of the art results. For a more thorough assessment of the effects of frequency and temporal attention in real-world conditions, we conduct ablation experiments by randomly dropping frequency bins and temporal frames from the input spectrograms, concluding that instead of modelling either of the entities, simultaneously modelling temporal and frequency attention translates to better real-world performance.
                </em>
              </font></div>
              <div id="icassp20_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @INPROCEEDINGS{9054440,
                  author={Yadav, Sarthak and Rai, Atul},
                  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
                  title={Frequency and Temporal Convolutional Attention for Text-Independent Speaker Recognition}, 
                  year={2020},
                  volume={},
                  number={},
                  pages={6794-6798},
                  doi={10.1109/ICASSP40776.2020.9054440}}
              </font></div>
            </td>
          </tr>

          <!-- INTERSPEECH 2018 - Discriminative speaker recog -->
          <tr onmouseout="inter18_0_stop()" onmouseover="inter18_0_start()">
            <td style="padding:25px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='inter18_0_image'><img src='images/pub_inter18_0_re.jpg'></div>
                <img src='images/pub_inter18_0_re.jpg'>
              </div>
              <script type="text/javascript">
                function inter18_0_start() {
                  document.getElementById('inter18_0_image').style.opacity = "1";
                }

                function inter18_0_stop() {
                  document.getElementById('inter18_0_image').style.opacity = "0";
                }
                inter18_0_stop()
              </script>
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/1015.pdf" target="_blank">
                <papertitle><font size="3">Learning Discriminative Features for Speaker Identification and Verification</font></papertitle>
              </a>
              <br>
              <font size="3">
              <strong><font size="3">Sarthak Yadav</font></strong>,
              <font size="3">Atul Rai</font>
              <br>
              <em>INTERSPEECH</em>, 2018
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('interspeech_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('interspeech_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="interspeech_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  The success of any Text Independent Speaker Identification and/or Verification system relies upon the system’s capability to learn discriminative features.
                  In this paper we propose a Convolutional Neural Network (CNN) Architecture based on the popular Very Deep VGG [1] CNNs, with key modifications to accommodate variable length spectrogram inputs, reduce the model disk space requirements and reduce the number of parameters, resulting in significant reduction in training times. We also propose a unified deep learning system for both Text-Independent Speaker Recognition and Speaker Verification, by training the proposed network architecture under the joint supervision of Softmax loss and Center loss [2] to obtain highly discriminative deep features that are suited for both Speaker Identification and Verification Tasks.
                  We use the recently released VoxCeleb dataset [3], which contains hundreds of thousands of real world utterances of over 1200 celebrities belonging to various ethnicities, for benchmarking our approach. Our best CNN model achieved a Top1 accuracy of 84.6%, a 4% absolute improvement over VoxCeleb’s approach, whereas training in conjunction with Center Loss improved the Top-1 accuracy to 89.5%, a 9% absolute improvement over Voxceleb’s approach.
                </em>
              </font></div>
              <div id="interspeech_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{yadav2018learning,
                  title="Learning Discriminative Features for Speaker Identification and Verification.",
                  author="Sarthak {Yadav} and Atul {Rai}",
                  booktitle="Interspeech 2018",
                  pages="2237--2241",
                  year="2018"
                }                
              </font></div>
            </td>
          </tr>


          <!-- Hindawi 2018 - UbiNets -->
          <tr onmouseout="hind18_0_stop()" onmouseover="hind18_0_start()">
            <td style="padding:25px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hind18_0_image'><img src='images/pub_hind_0_re.jpg'></div>
                <img src='images/pub_hind_0_re.jpg'>
              </div>
              <script type="text/javascript">
                function hind18_0_start() {
                  document.getElementById('hind18_0_image').style.opacity = "1";
                }

                function hind18_0_stop() {
                  document.getElementById('hind18_0_image').style.opacity = "0";
                }
                hind18_0_stop()
              </script>
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://downloads.hindawi.com/journals/afs/2018/5125103.pdf" target="_blank">
                <papertitle><font size="3">Prediction of Ubiquitination Sites Using UbiNets</font></papertitle>
              </a>
              <br>
              <font size="3">
              <strong><font size="3">Sarthak Yadav</font></strong>,
              <a href="https://www.smvdu.ac.in/index.php/bachelor-of-technology/35-faculty-profile/fp-comp-sc-engg/1223-dr-manoj-kumar-gupta" target="_blank"><font size="3">Manoj Kumar Gupta</font></a>,
              <a href="https://www.linkedin.com/in/ankur-singh-bist-5256a085" target="_blank"><font size="3">Ankur Singh Bist</font></a>
              <br>
              <em>Advances in Fuzzy Systems</em>, 2018
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('fuzzy_2018_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning" target="_blank"><font size="3">Code</font></a> / -->
              <!-- <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('fuzzy_2018_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="fuzzy_2018_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Ubiquitination controls the activity of various proteins and belongs to posttranslational modification. Various machine learning techniques are taken for prediction of ubiquitination sites in protein sequences. The paper proposes a new MLP architecture, named UbiNets, which is based on Densely Connected Convolutional Neural Networks (DenseNet). Computational machine learning techniques, such as Random Forest Classifier, Gradient Boosting Machines, and Multilayer Perceptrons (MLP), are taken for analysis. The main target of this paper is to explore the significance of deep learning techniques for the prediction of ubiquitination sites in protein sequences. Furthermore, the results obtained show that the newly proposed model provides significant accuracy. Satisfactory experimental results show the efficiency of proposed method for the prediction of ubiquitination sites in protein sequences. Further, it has been recommended that this method can be used to sort out real time problems in concerned domain
                </em>
              </font></div>
              <div id="fuzzy_2018_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{yadav2018prediction,
                  title={Prediction of ubiquitination sites using UbiNets},
                  author={Yadav, Sarthak and Gupta, Manoj and Bist, Ankur Singh},
                  journal={Advances in Fuzzy Systems},
                  volume={2018},
                  publisher={Hindawi}
                }
              </font></div>
            </td>
          </tr>

        </tbody></table> 

        <!-- Projects -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <br><br>
              <heading><font size="5">Projects</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- PROJECT - FSD50K baseline -->
          <tr onmouseout="proj_ser_stop()" onmouseover="proj_ser_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_ser_image'><img src='images/specs_wide2_re.jpg'></div>
                <img src='images/specs_wide2_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_ser_start() {
                  document.getElementById('proj_ser_image').style.opacity = "1";
                }

                function proj_ser_stop() {
                  document.getElementById('proj_ser_image').style.opacity = "0";
                }
                proj_ser_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/SarthakYadav/fsd50k-pytorch" target="_blank">
                <papertitle><font size="3">Sound event recognition on FSD50K</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              PyTorch implementation of the paper "<i>FSD50K: an Open Dataset of Human-Labeled Sound Events</i>", by Serra et al., 2020.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/SarthakYadav/fsd50k-pytorch" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/2010.00475" target="_blank"><font size="3">Paper</font></a>
              <p></p>
            </td>
          </tr>

        </tbody></table>

        <!-- Footer - Template Credits -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits : 
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
